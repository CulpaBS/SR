\documentclass[11pt]{article}
\usepackage[a4paper, hmargin={2.8cm, 2.8cm}, vmargin={2.5cm, 2.5cm}]{geometry}
\usepackage{eso-pic} % \AddToShipoutPicture
\usepackage{graphicx} % \includegraphics
\usepackage{framed}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath} % flere matematikkommandoer
\usepackage{amssymb} % flere matematikkommandoer
\usepackage{amsfonts}              % for blackboard bold, etc
\usepackage{amsthm}                % better theorem environments
\usepackage[utf8]{inputenc} % æøå
\usepackage[T1]{fontenc} % mere æøå
\usepackage{verbatim} % så man kan skrive ren tekst
\usepackage[all]{xy} % den sidste (avancerede) formel i dokumentet
\usepackage{graphicx}              % to include figures
\usepackage{caption}
\usepackage{bm}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{breqn}
\usepackage{tikz}
\usetikzlibrary{arrows,automata,fit,positioning, shapes}
\newcommand\doubleplus{+\kern-1.3ex+\kern0.8ex}
%% Change `ku-farve` to `nat-farve` to use SCIENCE's old colors or
%% `natbio-farve` to use SCIENCE's new colors and logo.
\def \ColourPDF {include/nat-farve}

%% Change `ku-en` to `nat-en` to use the `Faculty of Science` header
\def \TitlePDF   {include/nat-en}  % University of Copenhagen

\title{
  \vspace{3cm}
  \Huge{Implementing Map-Scan Fusion in the Futhark Compiler} \\
  \Large{Bachelor project}
}

\author{
  \Large{Brian Spiegelhauer}
  \\ \texttt{brianspieg@gmail.com} \\ \\
   \Large{William Jack Lysgaard Sprent}
  \\ \texttt{bsprent@gmail.com} \\
}

\date{
    \today
}

\begin{document}


\AddToShipoutPicture*{\put(0,0){\includegraphics*[viewport=0 0 700 600]{\ColourPDF}}}
\AddToShipoutPicture*{\put(0,602){\includegraphics*[viewport=0 600 700 1600]{\ColourPDF}}}

\AddToShipoutPicture*{\put(0,0){\includegraphics*{\TitlePDF}}}

\clearpage\maketitle
\thispagestyle{empty}

\newpage

\tableofcontents

\newpage

\section{Abstract}
\section{Introduction}
\textit{NOTE: the contents of this section is lifted from our synopsis and is probably placeholder}\\
The Futhark language is a functional programming with which the main idea is to allow for the expression of sufficiently complex programs while keeping complexity to a level where programs can be aggressively optimised and have their parallelism exploited \cite{futharkdoc}.

The Futhark compiler already supports a range of fusion optimisations \cite{T2Fusion}, but does not currently support fusion between \texttt{Map} and \texttt{Scan} statements.

For our project we will explore the possibility of implementing Map-Scan fusion into the Futhark compiler, and will examine the performance benefits (if any) of performing such optimisations.

\subsection{Motivation}
Fusion has the ``[..] potential to optimize both the memory hierarchy time overhead and, sometimes asymptotically, the space requirement" \cite{T2Fusion}. Hence the main motivation for adding Map-Scan fusion capabilities to the optimiser of the Futhark compiler, is the potential for enabling performance increases for some Futhark programs.

\subsection{Tasks}
The project can be divided into three main tasks:
\begin{enumerate}
    \item Gain an understanding of logical reasoning behind fusion optimisations on Second Order Array Combinators.
    \item Read and understand the relevant parts of the Futhark compiler required to make the necessary changes in the compiler.
    \item Modify all modules of the Futhark compiler necessary to implement the Map-Scan fusion itself.
\end{enumerate}
At first sight, these tasks look fairly straight forward. However, we expect that the main difficulties of this project lie within unforeseen roadblocks we will run into when modifying the codebase.

\section{Background Information}
%Describe some relevant background info for how SOACs are parallelly computed - relevant %to why Scanomap is smart. IMPORTANT: WHY MEMORY MANAGEMENT IS VERY IMPORTANT ON GPU\\

There are problems/calculations that gets to a size where normal sequential programming involving consecutive execution of processes, will reach a computation time unsatisfying for the intended users. In some cases these calculations can be done much faster with parallel programming. Parallel programming is where many calculations are carried out simultaneously, with the idea of dividing a problem into smaller sub problems solved at the same time. Parallel and sequential programming are not mutually exclusive, in the sense that if you use parallel programming, you cant use sequential programming, in many cases they are used together. Parallel programming can be done on the CPU with its multiple cores, but when possible and advantages it is much better to harness the thousands of cores in the GPU - graphics processing units. The GPU is no longer only used to do graphical calculations, but also General-purpose computing, GPGPU (General-purpose computing on graphics processing units). \\

To do GPGPU, Hyperfit a joint research center addressing the simultaneous challenges of high transparency, high computational performance and high productivity in finance, employing an integrated approach of financial mathematics, domain specific languages, parallel functional programming, and high-performance systems \cite{Hyperfit} created Futhark. 


\subsection{Futhark}
As described in Troels Henriksens master thesis \cite[The $\mathcal{L}_0$ language, p. 8]{MasterTroels} the language $\mathcal{L}_0$ later renamed Futhark is in a sense "sufficient", in that it is Turing-complete, and can express imperative style loops with do-loops. However Futhark is ment to use second-order array combinations (SOACs) to do bulk operations on arrays instead of using the do-loops. In this sections the reasoning behind using SOACs will be explained by showing the difference in their computation when done sequentially vs. parallelly.
%Chunking
\subsection{SOACs}
Both \texttt{map} and \texttt{scan} are defined as SOACs -- or Second Order Array Combinators. Hence they have no free variables, take first-order functions as arguments, and output first-order
 functions whose domains are arrays of the domain of the input. Furthermore, in Futhark, these array inputs and outputs are tuples of arrays, and not arrays of tuples. Working with SOACs allows for some assumptions to be made which turn out to be useful in regards to both parallisation and optimisation. In particular each SOAC can be considered as representing a specific shape of an imperative do-loop, which
 is used in Futhark to expedite loop-fusion. \cite[chap. 7]{MasterTroels}

% Definition of SOACs
% Why are they useful wrt. parallelisation
% Soacs with tuples in Futhark
\subsubsection{Map}
The $\texttt{map} \: f \: a$ function, has the very simple definition of taking a function $f \: : \: \alpha \to \beta$ and returning a function $\mathtt{map} \:f \: : \: [\alpha] \to [\beta]$  which
 applies $f$ to every element of an input array, $a$.  This gives us the type signature of \texttt{map},
$$\mathtt{map} \: f \: a \: :  \: (\alpha \to \beta) \to [\alpha] \to [\beta]\mathnormal{.}$$
bAnd the semantic definition of \texttt{map},
$$\mathtt{map} \: f \: a \: =  \: [f(a_0), f(a_1), ..., f(a_{n-1})]\mathnormal{.}$$
Having no free variables, means that each result $f(a_i)$ \textit{only} depends on the corresponding element $a_i$. This makes \texttt{map}s fantastic for parallelisation as once the
 degree of parallism reaches the size of $a$, $\mathtt{map} \: f \: a$ can be potentially be computed in a single parrallel step, or $c$ steps for a chunk size of $c$.

% INSERT COOL FIGURE OF MAP BEING COMPUTED 
% How do we compute it parallely
% Timecomplexity?

\subsubsection{Scan}
% what is a scan
$\texttt{scan} \: \odot \: e \: a$ takes a binary, associative function $\odot \: : \: \alpha \to \alpha \to \alpha$ and returns a function
 $\mathtt{scan} \:\odot \: : \: \alpha \to [\alpha] \to [\alpha]$ which
 computes the $\odot$ prefixes of an input array $a$ starting with a neutral element, $e$. Overall, \texttt{scan} has the type signature,
$$\mathtt{scan} \: \odot \: e \: a \: : \:(\alpha \to \alpha \to \alpha) \to \alpha \to [\alpha] \to [\alpha]\mathnormal{.}$$
Computing \texttt{scan} with the function $\odot$, the array $a$, and neutral element $e$ gives us,
$$\mathtt{scan} \: \odot \: e \: a \: = [e \odot a_0, e \odot a_0 \odot a_1, ..., e \odot a_0 \odot ... \odot a_{n-1}]\mathnormal{.}$$
% How do we compute it parallely
However, computing such a \texttt{scan} is not as simple as with a \texttt{map} as each prefix $a_0 \odot ... \odot a_i$ obviously depends on the previous prefix $a_0 \odot ... \odot a_{i-1}$. Hence, 
the associativity of $\odot$ is vital as it means that this dependency does not force computation order, and partial results can be computed independently and combined.
% How scan is computed in Futhark with fancy diagram 

% Timecomplexity?

\subsection{Parallel Computations on GPGPUs}
%% Spørg troels om baggrundsinfo

\subsubsection{Memory Conditions}
Small cache high miss penalty.


\section{Map-Scan Fusion}
% Why?
In a typical situation involving \texttt{map} and \texttt{scan}, we will have the \texttt{map} producing some array, which is subsequently
 consumed by the \texttt{scan}:
\begin{align*}
  b &= \mathtt{map} \: f \: a \\
  c &= \mathtt{scan} \: \odot \: e \: b\mathnormal{.}
\end{align*}
In this situation, the memory access pattern will look something like the following,
\begin{align*}
  \mathtt{load}[a_0]&, \mathtt{store}[b_0], \mathtt{load}[a_1], \mathtt{store}[b_1], ..., \mathtt{load}[a_{n-1}], \mathtt{store}[b_{n-1}] \\
  \mathtt{load}[b_0]&, \mathtt{store}[c_0], \mathtt{load}[b_1], \mathtt{store}[c_1], ..., \mathtt{load}[b_{n-1}], \mathtt{store}[c_{n-1}]\mathnormal{.}
\end{align*}
Recalling that element of the scanned array $c$ depend only on the previous prefixes, we have that $c_i$ will only depend on $b_j$ for $j \leq i$ being calculated.
 This means that, ideally, we can interleave this access pattern without breaking any dependencies -- giving us a pattern more like this:
 $$\mathtt{load}[a_0], \mathtt{store}[b_0], \mathtt{load}[b_0], \mathtt{store}[c_0], ..., \mathtt{load}[a_{n-1}], \mathtt{store}[b_{n-1}], \mathtt{load}[b_{n-1}], \mathtt{store}[c_{n-1}]\mathnormal{.}$$
In this case, all accesses to $b$ become wholly uneccessary and can be removed, giving us:
 $$\mathtt{load}[a_0],  \mathtt{store}[c_0], ..., \mathtt{load}[a_{n-1}],  \mathtt{store}[c_{n-1}]\mathnormal{.}$$
This is the goal of perfoming \texttt{map}-\texttt{scan} fusion. By fusing a \texttt{map} into a \texttt{scan} we can optimise a program by eliminating costly memory accesses.
 Something which is even more useful when working on a GPGPU, which generally do not have the more forgiving memory hierarchy and caching system of a CPU.
\subsection{Scanomap}
% Naive approach
When looking to fuse a \texttt{map} into a \texttt{scan}, the most straight forward approach is to attempt to perform function
 composition on the input functions of the respective functions. Hence, turning the following
\begin{align}
  b &= \mathtt{map} \: f \: a \\
  c &= \mathtt{scan} \: \odot \: e \: b
\intertext{into,}
  c &= \mathtt{scan} \: \odot_f \: e \: a
\end{align}
where, $$x \odot_f y = \odot \circ f = x \odot f(y)$$ would be the naive approach.
However, the type signature of the resulting function $$\odot_f \: : \: \alpha \to \beta \to \alpha$$
 is not compatible with the Futhark definition of \texttt{scan}, and neither is it associative. Clearly, a different approach
 is needed.

The solution to this problem is to exploit how Futhark uses chunking. Since the associativity of the \texttt{scan} operator only
 comes into play during the parallel phase of computation, we can distiguish between how each chunk is computed sequentially and
 how chunks are joined.

To do so, we must expand the list of SOACs with the internal \texttt{scanomap} function. \texttt{scanomap} is semantically similar
 to \texttt{scan}, however it takes two function parameters -- an associatve scanning function meant for parallelly scanning across
 chunks, and a sequential folding function meant for scanning within a chunk. Scanomap has the following type signature,
$$\mathtt{scanomap} \: \odot \: \odot_f \: e \: a \: : \:(\alpha \to \alpha \to \alpha) \to (\alpha \to \beta \to \alpha)
 \to \alpha \to [\beta] \to [\alpha]\mathnormal{.}$$
and can be considered semantically similar to performing a left fold with the $\odot_f$ function
$$\mathtt{scanomap} \: \odot \: \odot_f \: e \: a \: =
 [e \odot_f a_0, (e \odot_f a_0) \odot_f a_1, ..., ((e \odot_f a_0) \odot_f ...) \odot_f a_{n-1}]$$
which also corresponds to the chuck-wise computation of \texttt{scanomap}. The scanning operator can then be used to
 join two chunks,
$$\mathtt{scanomap} \: \odot \: \odot_f \: e \: (a \doubleplus b) \: = 
[a_0', a_1', ..., a_{n-1}'] \doubleplus [b_0' \odot a_{n-1}',b_1' \odot a_{n-1}', ...,b_{n-1}' \odot a_{n-1}']$$
where 
\begin{align*}
  \mathtt{scanomap} \: \odot \: \odot_f \: e \: a \: &= 
[a_0', a_1', ..., a_{n-1}'] \\
  \mathtt{scanomap} \: \odot \: \odot_f \: e \: b \: &= 
[b_0', b_1', ..., b_{n-1}']\mathnormal{.}
\end{align*}

For \texttt{scanomap} to be used in facilitating \texttt{map}-\texttt{scan} fusion, we first observe that we have the
 following equivalence,
$$\mathtt{scan} \: \odot \: e \: a \equiv \mathtt{scanomap} \: \odot \: \odot \: e \: a \mathnormal{.}$$
Hence we can freely turn any regular \texttt{scan} into an equivalent \texttt{scanomap}. If we once again take a look at our previous example
 we can see how we can now fuse a \texttt{map} into a \texttt{scan} using the \texttt{scanomap} construction:
\begin{align*}
  b &= \mathtt{map} \: f \: a \\
  c &= \mathtt{scan} \: \odot \: e \: b
\intertext{Firstly, we turn the \texttt{scan} into an equivalent \texttt{scanomap},}
  c &= \mathtt{scanomap} \: \odot \: \odot \: e \: b
\intertext{and then compose a folding function, $\odot \circ f = \odot_f$, from the mapping function and the scanning operator and discard
 the intermediate $b$ list, to arrive at an equivalent \texttt{scanomap},}
  c &= \mathtt{scanomap} \: \odot \: \odot_f \: e \: a\mathnormal{.}
\end{align*}
Any producer \texttt{map} can be fused into a consuming \texttt{scan} in this manner.
% Why do we need the scanomap construction?
% What is the Scanomap construction. What are its semantics, and why is it used.
% Show equivalence between a Scanomap and a composition of a map and a scan - similar to showing redomap results from a reduce . map.

\subsection{Necessary Conditions}
% Copy from T2/Troels.
What are the conditions for a scan map fusion. When can we fuse, and when can we not fuse.
Our Scanomap supports carrying outputs from the map - explain why and how.
\subsection{Fusing Scanomap}
% What can we not reduce with
%horizontally
%maposcanomap
\subsection{Fusion Strategy/Example}
Walk through the entire fusion process concisely - or similarly summarize the relevant T2 Graph reduction fusion paper rooted in scan.
Show the fusion of an example instance of maps and scans through dependency graphs etc.

\section{Implementation}

What is the state of the Futhark codebase at project start. How is Scan currently handled. What is already there, and what do we need to implement.\\
Our solution is closely related to how Redomap fusion is handled. How does it differ.\\
What parts of the code have we touched and why.\\
Describe how different parts of the Map-Scan fusion is done - e.g. describe how function composition is implemented and so on.


\section{Benchmarking and Testing}
How have we tested our implementation. Does it work? Why?\\
How does the performance of a fused program compare with a non-fused program both sequentially and parallelly. Why?

\section{Conclusion}
\newpage

\bibliographystyle{unsrt}
\bibliography{lit}

\end{document}